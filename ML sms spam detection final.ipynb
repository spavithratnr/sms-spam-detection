{"cells":[{"cell_type":"markdown","metadata":{"id":"avr5ozljVG_m"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"qX5h_KEbVG_p","executionInfo":{"status":"ok","timestamp":1644908072025,"user_tz":-330,"elapsed":1042,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}}},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import time\n","import warnings\n","import numpy as np\n","from sklearn.preprocessing import normalize\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.manifold import TSNE\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","import math\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","from sklearn import model_selection\n","from sklearn.ensemble import RandomForestClassifier\n","\n","import pickle\n"]},{"cell_type":"markdown","metadata":{"id":"pyJ8Ro90VG_t"},"source":["## Importing models and vectorizers"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1486,"status":"ok","timestamp":1644908264430,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"},"user_tz":-330},"id":"yYIX13L9WLu4","outputId":"1d704818-75ee-423e-e57a-75a4596a0c3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-02-15 06:57:42--  https://doc-14-4k-docs.googleusercontent.com/docs/securesc/lo66ka9s3k9ovif7kku17kum7962ok4j/2q26j9euag6l3gho3krv49ecrvk06bm8/1644908175000/08255761277798433337/08255761277798433337/1mw_EPvksRBgOQO2ELw8ZofOXQ5FBzOoD?e=download&authuser=0\n","Resolving doc-14-4k-docs.googleusercontent.com (doc-14-4k-docs.googleusercontent.com)... 108.177.11.132, 2607:f8b0:400c:c01::84\n","Connecting to doc-14-4k-docs.googleusercontent.com (doc-14-4k-docs.googleusercontent.com)|108.177.11.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1716637 (1.6M) [application/zip]\n","Saving to: ‘sms_spam_detection_vectorisers.zip’\n","\n","sms_spam_detection_ 100%[===================>]   1.64M  --.-KB/s    in 0.01s   \n","\n","2022-02-15 06:57:43 (124 MB/s) - ‘sms_spam_detection_vectorisers.zip’ saved [1716637/1716637]\n","\n"]}],"source":["!wget --header=\"Host: doc-14-4k-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" --header=\"Cookie: AUTH_mm5pe75pf4mhpbdikb2k80b74s62g311=08255761277798433337|1644908025000|1vac2lt4gnquk0nmpg1ki1drj7ihkrlb\" --header=\"Connection: keep-alive\" \"https://doc-14-4k-docs.googleusercontent.com/docs/securesc/lo66ka9s3k9ovif7kku17kum7962ok4j/2q26j9euag6l3gho3krv49ecrvk06bm8/1644908175000/08255761277798433337/08255761277798433337/1mw_EPvksRBgOQO2ELw8ZofOXQ5FBzOoD?e=download&authuser=0\" -c -O 'sms_spam_detection_vectorisers.zip'"]},{"cell_type":"code","source":["!unzip sms_spam_detection_vectorisers.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHGxkPJanxTc","executionInfo":{"status":"ok","timestamp":1644908268802,"user_tz":-330,"elapsed":402,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}},"outputId":"90c909fc-df99-48c0-f6bb-9f34f47d21f0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  sms_spam_detection_vectorisers.zip\n","  inflating: best_rf_model.pkl       \n","  inflating: __MACOSX/._best_rf_model.pkl  \n","  inflating: scaler.pkl              \n","  inflating: __MACOSX/._scaler.pkl   \n","  inflating: tf_idf_vectorizer.pkl   \n","  inflating: __MACOSX/._tf_idf_vectorizer.pkl  \n"]}]},{"cell_type":"markdown","source":["## Pre-process data"],"metadata":{"id":"j86hxXF3YcxQ"}},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":true,"id":"zHOhbUYWVG_9","executionInfo":{"status":"ok","timestamp":1644908365687,"user_tz":-330,"elapsed":918,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"55760b6c-c631-4e0c-d9c8-36ddb0bc9843"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["# Ref: https://stackoverflow.com/questions/22132525/add-column-with-number-of-days-between-dates-in-dataframe-pandas\n","def decontractions(phrase):\n","    \"\"\"decontracted takes text and convert contractions into natural form.\n","     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n","    # specific\n","    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n","\n","    # general\n","    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n","\n","    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n","\n","    return phrase\n","\n","\n","\n","import string\n","#defining the function to remove punctuation\n","def remove_punctuation(text):\n","    punctuation_free=\"\".join([i for i in text if i not in string.punctuation])\n","    return punctuation_free\n","\n","\n","\n","\n","\n","#defining function for tokenization\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n"," \n","def tokenization(text):\n","    # tokens = re.split('W+',text)\n","    tokens = word_tokenize(text)\n","    return tokens\n","\n","\n","\n","#importing nlp library\n","import nltk\n","nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","\n","#defining the function to remove stopwords from tokenized text\n","def remove_stopwords(text):\n","    \n","    output= [i for i in text if i not in stopwords]\n","    # print(output)\n","    return output\n","\n","\n","\n","\n","\n","\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","#defining the object for Lemmatization\n","wordnet_lemmatizer = WordNetLemmatizer()\n","#defining the function for lemmatization\n","def lemmatizer(text):\n","  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n","  return lemm_text\n","\n","\n","\n","\n","\n","\n","\n","def preprocess_data(sms_text):\n","\n","  sms_spam_data = pd.DataFrame()\n","  sms_spam_data['original_text'] = [sms_text]\n","  sms_spam_data['clean_msg']= sms_spam_data['original_text'].apply(lambda x: decontractions(x))\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x:remove_punctuation(x))\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x: x.lower())\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x: tokenization(x))\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x:remove_stopwords(x))\n","  sms_spam_data['clean_msg']=sms_spam_data['clean_msg'].apply(lambda x:lemmatizer(x))\n","  sms_spam_data['clean_msg_length'] = sms_spam_data[\"clean_msg\"].str.len()\n","\n","  return sms_spam_data"]},{"cell_type":"markdown","metadata":{"id":"eSNBdcixVHA1"},"source":["## Featurisation"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n"],"metadata":{"id":"uZBp2Vvd23aV","executionInfo":{"status":"ok","timestamp":1644908369533,"user_tz":-330,"elapsed":1179,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def featurize(sms_spam_data):\n","  sms_spam_data[\"clean_msg\"] = sms_spam_data[\"clean_msg\"].map(' '.join)\n","  sms_spam_data = featurize_clean_msg(sms_spam_data)\n","  # print(sms_spam_data.shape)\n","  sms_spam_data = featurize_clean_msg_length(sms_spam_data)\n","  # print(sms_spam_data.shape)\n","  sms_spam_data = sms_spam_data.drop(['original_text', 'clean_msg', 'clean_msg_length'], axis = 1)\n","  \n","  # print(sms_spam_data.shape)\n","  return sms_spam_data\n","\n","  \n"],"metadata":{"id":"bIZCkkqIecMb","executionInfo":{"status":"ok","timestamp":1644908370676,"user_tz":-330,"elapsed":15,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5JzJhrAVHA2"},"source":["### Applying tfidf vectorizer to sms text feature"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Kwg73bJMVHA3","executionInfo":{"status":"ok","timestamp":1644908372851,"user_tz":-330,"elapsed":338,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}}},"outputs":[],"source":["# Ref - https://stackoverflow.com/questions/43601358/empty-vocabulary-for-single-letter-by-countvectorizer\n","def featurize_clean_msg(sms_spam_data):\n","\n","  tf_idf = pickle.load(open(\"tf_idf_vectorizer.pkl\", 'rb'))\n","  # tf_idf = TfidfVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\")\n","  sms_tf_idf = tf_idf.transform(sms_spam_data['clean_msg'])\n","  clean_msg_features = tf_idf.get_feature_names()\n","  print(sms_spam_data.columns)\n","  print(sms_tf_idf.shape)\n","  sms_spam_data[clean_msg_features] = pd.DataFrame(sms_tf_idf.todense(), index=sms_spam_data.index)\n","  print(\"In featurize_clean_msg after\")\n","  print(sms_spam_data.shape)\n","  return sms_spam_data"]},{"cell_type":"markdown","metadata":{"id":"IIx7Mg09VHA7"},"source":["### Applying min max scaler on sms length"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1130,"status":"ok","timestamp":1644908375814,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"},"user_tz":-330},"id":"RoeJt1z7VHA8"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","def featurize_clean_msg_length(sms_spam_data):\n","    scaler = pickle.load(open(\"scaler.pkl\", 'rb'))\n","    # scaler = MinMaxScaler()\n","    # scaler.fit(sms_spam_data['clean_msg_length'].values.reshape(-1, 1))\n","    sms_spam_data['nrm_clean_msg_length']=scaler.transform(sms_spam_data['clean_msg_length'].values.reshape(-1, 1))\n","\n","    return sms_spam_data\n"]},{"cell_type":"markdown","source":["## Predict function"],"metadata":{"id":"NPtmKGqviiuM"}},{"cell_type":"code","source":["def predict_spam(sms_spam_data):\n","  model = pickle.load(open(\"best_rf_model.pkl\", 'rb'))\n","  return model.predict(sms_spam_data)"],"metadata":{"id":"DLUREPaRioT7","executionInfo":{"status":"ok","timestamp":1644908377285,"user_tz":-330,"elapsed":1118,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## Final function"],"metadata":{"id":"12UNlTGMWB2R"}},{"cell_type":"code","source":["import time\n","\n","def final(sms_text):\n","  \n","  start = time.time()\n","  \n","  \n","  # pre-process data\n","  sms_spam_data = preprocess_data(sms_text)\n","\n","  # featurize data\n","  sms_spam_data = featurize(sms_spam_data)\n","  # predict\n","  spam = predict_spam(sms_spam_data)\n","\n","  end = time.time()\n","  print(\"Took {0} seconds to predict\".format(end - start))\n","\n","  print(\"Spam or not?\", spam[0])\n","\n","  return spam[0]\n"],"metadata":{"id":"2j1ceHCDWACO","executionInfo":{"status":"ok","timestamp":1644908516368,"user_tz":-330,"elapsed":365,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["final(\"hurray! you won $20 reward!\")"],"metadata":{"id":"W-iD0ZZNyR0D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644908524986,"user_tz":-330,"elapsed":6821,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}},"outputId":"16c0e096-d1e4-47de-abcd-116121867989"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['original_text', 'clean_msg', 'clean_msg_length'], dtype='object')\n","(1, 7849)\n","In featurize_clean_msg after\n","(1, 7852)\n","Took 7.252431154251099 seconds to predict\n","Spam or not? 0\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["### Deploying web app using streamlit"],"metadata":{"id":"8Fe1us96MKe9"}},{"cell_type":"code","source":["!pip install streamlit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"JsMm1UhqMO8a","executionInfo":{"status":"ok","timestamp":1644909474941,"user_tz":-330,"elapsed":17600,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}},"outputId":"c0ff6e32-855b-482d-f8ef-8d95433be53f"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.5.1-py2.py3-none-any.whl (9.7 MB)\n","\u001b[K     |████████████████████████████████| 9.7 MB 5.1 MB/s \n","\u001b[?25hCollecting watchdog\n","  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n","Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n","Collecting blinker\n","  Downloading blinker-1.4.tar.gz (111 kB)\n","\u001b[K     |████████████████████████████████| 111 kB 58.2 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.10.0.2)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.4.0)\n","Collecting gitpython!=3.1.19\n","  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 61.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 48.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n","Collecting toml\n","  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n","Collecting pympler>=0.9\n","  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n","\u001b[K     |████████████████████████████████| 164 kB 55.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n","Collecting base58\n","  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n","Collecting validators\n","  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n","Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (4.10.1)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.5)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n","Collecting ipykernel>=5.1.2\n","  Downloading ipykernel-6.9.0-py3-none-any.whl (128 kB)\n","\u001b[K     |████████████████████████████████| 128 kB 46.7 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.4)\n","Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n","Collecting ipython>=7.23.1\n","  Downloading ipython-7.31.1-py3-none-any.whl (792 kB)\n","\u001b[K     |████████████████████████████████| 792 kB 41.4 MB/s \n","\u001b[?25hRequirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n","\u001b[K     |████████████████████████████████| 380 kB 57.0 MB/s \n","\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.2)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.3.0)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.9.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.1.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n","Building wheels for collected packages: blinker\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=e2ccb121113c72c4a148310afb69af3ac36e811051c694857e320f5f9bf6014f\n","  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n","Successfully built blinker\n","Installing collected packages: prompt-toolkit, ipython, ipykernel, smmap, gitdb, watchdog, validators, toml, pympler, pydeck, gitpython, blinker, base58, streamlit\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n","google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.0 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.31.1 which is incompatible.\u001b[0m\n","Successfully installed base58-2.1.1 blinker-1.4 gitdb-4.0.9 gitpython-3.1.26 ipykernel-6.9.0 ipython-7.31.1 prompt-toolkit-3.0.28 pydeck-0.7.1 pympler-1.0.1 smmap-5.0.0 streamlit-1.5.1 toml-0.10.2 validators-0.18.2 watchdog-2.1.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","ipykernel","prompt_toolkit"]}}},"metadata":{}}]},{"cell_type":"code","source":["%%writefile sms_app.py\n","import streamlit as st\n","import time\n","import os\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import string\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","\n","\n","\n","def final(sms_text):\n","  \n","  start = time.time()\n","  \n","  \n","  # pre-process data\n","  sms_spam_data = preprocess_data(sms_text)\n","\n","  # featurize data\n","  sms_spam_data = featurize(sms_spam_data)\n","  # predict\n","  spam = predict_spam(sms_spam_data)\n","\n","  end = time.time()\n","  print(\"Took {0} seconds to predict\".format(end - start))\n","\n","  print(\"Spam or not?\", spam[0])\n","\n","  return spam[0]\n","\n","\n","\n","\n","\n","\n","  # Ref: https://stackoverflow.com/questions/22132525/add-column-with-number-of-days-between-dates-in-dataframe-pandas\n","def decontractions(phrase):\n","    \"\"\"decontracted takes text and convert contractions into natural form.\n","     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n","    # specific\n","    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n","\n","    # general\n","    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n","\n","    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n","\n","    return phrase\n","\n","\n","\n","#defining the function to remove punctuation\n","def remove_punctuation(text):\n","    punctuation_free=\"\".join([i for i in text if i not in string.punctuation])\n","    return punctuation_free\n","\n","\n","\n","\n","\n","#defining function for tokenization\n","nltk.download('punkt')\n","\n"," \n","def tokenization(text):\n","    # tokens = re.split('W+',text)\n","    tokens = word_tokenize(text)\n","    return tokens\n","\n","\n","\n","#importing nlp library\n","nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","\n","#defining the function to remove stopwords from tokenized text\n","def remove_stopwords(text):\n","    \n","    output= [i for i in text if i not in stopwords]\n","    # print(output)\n","    return output\n","\n","\n","\n","\n","\n","\n","nltk.download('wordnet')\n","\n","#defining the object for Lemmatization\n","wordnet_lemmatizer = WordNetLemmatizer()\n","#defining the function for lemmatization\n","def lemmatizer(text):\n","  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n","  return lemm_text\n","\n","\n","\n","\n","\n","\n","\n","def preprocess_data(sms_text):\n","\n","  sms_spam_data = pd.DataFrame()\n","  sms_spam_data['original_text'] = [sms_text]\n","  sms_spam_data['clean_msg']= sms_spam_data['original_text'].apply(lambda x: decontractions(x))\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x:remove_punctuation(x))\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x: x.lower())\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x: tokenization(x))\n","  sms_spam_data['clean_msg']= sms_spam_data['clean_msg'].apply(lambda x:remove_stopwords(x))\n","  sms_spam_data['clean_msg']=sms_spam_data['clean_msg'].apply(lambda x:lemmatizer(x))\n","  sms_spam_data['clean_msg_length'] = sms_spam_data[\"clean_msg\"].str.len()\n","\n","  return sms_spam_data\n","\n","\n","\n","def featurize(sms_spam_data):\n","  sms_spam_data[\"clean_msg\"] = sms_spam_data[\"clean_msg\"].map(' '.join)\n","  sms_spam_data = featurize_clean_msg(sms_spam_data)\n","  # print(sms_spam_data.shape)\n","  sms_spam_data = featurize_clean_msg_length(sms_spam_data)\n","  # print(sms_spam_data.shape)\n","  sms_spam_data = sms_spam_data.drop(['original_text', 'clean_msg', 'clean_msg_length'], axis = 1)\n","  \n","  # print(sms_spam_data.shape)\n","  return sms_spam_data\n","\n","  \n","\n","# Ref - https://stackoverflow.com/questions/43601358/empty-vocabulary-for-single-letter-by-countvectorizer\n","def featurize_clean_msg(sms_spam_data):\n","\n","  tf_idf = pickle.load(open(\"tf_idf_vectorizer.pkl\", 'rb'))\n","  # tf_idf = TfidfVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\")\n","  sms_tf_idf = tf_idf.transform(sms_spam_data['clean_msg'])\n","  clean_msg_features = tf_idf.get_feature_names()\n","  #print(sms_spam_data.columns)\n","  #print(sms_tf_idf.shape)\n","  sms_spam_data[clean_msg_features] = pd.DataFrame(sms_tf_idf.todense(), index=sms_spam_data.index)\n","  #print(\"In featurize_clean_msg after\")\n","  #print(sms_spam_data.shape)\n","  return sms_spam_data\n","\n","\n","\n","\n","\n","def featurize_clean_msg_length(sms_spam_data):\n","    scaler = pickle.load(open(\"scaler.pkl\", 'rb'))\n","    # scaler = MinMaxScaler()\n","    # scaler.fit(sms_spam_data['clean_msg_length'].values.reshape(-1, 1))\n","    sms_spam_data['nrm_clean_msg_length']=scaler.transform(sms_spam_data['clean_msg_length'].values.reshape(-1, 1))\n","\n","    return sms_spam_data\n","\n","\n","\n","\n","def predict_spam(sms_spam_data):\n","  model = pickle.load(open(\"best_rf_model.pkl\", 'rb'))\n","  return model.predict(sms_spam_data)\n","\n","\n","\n","\n","\n","def main():\n","  st.title(\"SMS Spam Detection\")\t\n","  text = st.text_input(\"Enter Your Text\", \"\")\n"," \n","# display the name when the submit button is clicked\n","# .title() is used to get the input text string\n","  if(st.button('Submit')):\n","    sms_text = text.title()\n","    spam_detection = final(sms_text)\n","    if(spam_detection == 0):\n","      st.success(\"Not a spam message\")\n","    else:\n","      st.error(\"It is a spam message\")  \n"," \n","  \n","if __name__ == '__main__':\n","  main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUUH7yO0MZnn","executionInfo":{"status":"ok","timestamp":1644909833808,"user_tz":-330,"elapsed":435,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}},"outputId":"1e097599-de63-448d-a14e-fc079a22215e"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting sms_app.py\n"]}]},{"cell_type":"code","source":["!streamlit run sms_app.py --server.enableWebsocketCompression=false & npx localtunnel --port 8501 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8Z16dyiP3ZE","executionInfo":{"status":"ok","timestamp":1644909962154,"user_tz":-330,"elapsed":125603,"user":{"displayName":"Pavithra S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhanCo8VJCB_zYmwQJ025L7jqQmJEJofOH-cArLTQ=s64","userId":"08255761277798433337"}},"outputId":"5f211a27-0383-4c4a-8bbb-4a975993090d"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-02-15 07:23:57.555 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.897s\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.243.210.78:8501\u001b[0m\n","\u001b[0m\n","your url is: https://moody-lion-97.loca.lt\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  self[k1] = value[k2]\n","/content/sms_app.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  sms_spam_data['nrm_clean_msg_length']=scaler.transform(sms_spam_data['clean_msg_length'].values.reshape(-1, 1))\n","Took 8.524081468582153 seconds to predict\n","Spam or not? 1\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  self[k1] = value[k2]\n","/content/sms_app.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n","  sms_spam_data['nrm_clean_msg_length']=scaler.transform(sms_spam_data['clean_msg_length'].values.reshape(-1, 1))\n","Took 6.620622396469116 seconds to predict\n","Spam or not? 0\n","\u001b[34m  Stopping...\u001b[0m\n","^C\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ML sms spam detection final.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":0}